{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Notebook Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/imran/Workspace/llm-agent-from-scratch\n"
     ]
    }
   ],
   "source": [
    "# Stay on the root directory\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Returns True if at least one environment variable is set else False\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.list().models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "llm = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OLLAMA_BASE_URL\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user asked, \"What was my last question? and your answer?\" So I need to figure out what their last question was and my previous answer. Let me check the conversation history.\n",
       "\n",
       "Looking back, the user's first question was about the capital of France. I answered that Paris is the capital. Then the user asked about their last question and my answer. So their last question was the one about the capital, and my answer was Paris. \n",
       "\n",
       "Wait, but the user is now asking for their last question and my answer. So I should confirm that their last question was indeed the first one, and my answer was Paris. I need to make sure I'm not missing any other questions in between. Since there's only two messages, the user's last question is the one about France's capital, and my answer was Paris. So I should respond by restating that their last question was about the capital of France and my answer was Paris. I should present it clearly, maybe in bullet points as they did before. Also, make sure to mention that they can ask more questions if needed.\n",
       "</think>\n",
       "\n",
       "Your last question was:  \n",
       "**\"What is the capital of France?\"**  \n",
       "\n",
       "My answer was:  \n",
       "**The capital of France is Paris.**  \n",
       "\n",
       "Let me know if you have more questions! ðŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "    model=os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "\n",
    "messages.append({\"role\":\"assistant\", \"content\":response.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"What was my last question? and your answer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the capital of France?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<think>\\nOkay, the user is asking for the capital of France. Let me think. First, I need to make sure I remember correctly. France is a country in Europe, and I'm pretty sure the capital is Paris. But wait, I should double-check to avoid any mistakes. Have there been any changes recently? I don't think so. The capital cities are usually well-established. Paris is known for its landmarks like the Eiffel Tower, the Louvre Museum, and the Seine River. Also, it's a major cultural and economic hub. I believe there's no other city that's considered the capital of France. Maybe some people confuse it with other cities like Lyon or Marseille, but those are major cities, not capitals. So I'm confident the answer is Paris. Just to recap: France's capital is Paris. No doubt there.\\n</think>\\n\\nThe capital of France is **Paris**. It is a major cultural, economic, and political center in Europe, known for landmarks like the Eiffel Tower, the Louvre Museum, and the Seine River.\"},\n",
       " {'role': 'user', 'content': 'What was my last question? and your answer?'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "\n",
    "completion = llm.beta.chat.completions.parse(\n",
    "    model=os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: First, we need to isolate the term with the variable x. To do this, subtract 7 from both sides of the equation to get rid of the constant term on the left side.\n",
      "Output: 8x + 7 - 7 = -23 - 7\n",
      "\n",
      "---\n",
      "\n",
      "Step 2: Simplify both sides of the equation by performing the subtraction. On the left side, 7 - 7 cancels out, leaving 8x. On the right side, -23 - 7 equals -30.\n",
      "Output: 8x = -30\n",
      "\n",
      "---\n",
      "\n",
      "Step 3: Now, we need to solve for x by getting rid of the coefficient 8 that is multiplied by x. To do this, divide both sides of the equation by 8.\n",
      "Output: x = -30 / 8\n",
      "\n",
      "---\n",
      "\n",
      "Step 4: Simplify the fraction by dividing both the numerator and the denominator by their greatest common divisor, which is 2. This reduces the fraction to its simplest form.\n",
      "Output: x = -15 / 4\n",
      "\n",
      "---\n",
      "\n",
      "Step 5: The final answer is x equals negative fifteen fourths. This is the exact value of x that satisfies the original equation.\n",
      "Output: x = -15/4\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "\n",
    "completion = llm.beta.chat.completions.parse(\n",
    "    model=os.getenv(\"LLM_MODEL_NAME\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x+7=-23\"},\n",
    "    ],\n",
    "    response_format=MathReasoning,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "math_reasoning = completion.choices[0].message.parsed\n",
    "\n",
    "for i, step in enumerate(math_reasoning.steps):\n",
    "    print(f\"Step {i+1}: {step.explanation}\")\n",
    "    print(f\"Output: {step.output}\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(f\"Final Answer: {math_reasoning.final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factchecking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
